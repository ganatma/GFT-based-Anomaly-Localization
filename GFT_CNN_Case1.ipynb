{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ac1ac725",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:45<?, ?it/s]\n"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": "Exception encountered when calling layer 'conv1d' (type Conv1D).\n\n{{function_node __wrapped__BiasAdd_device_/job:localhost/replica:0/task:0/device:CPU:0}} OOM when allocating tensor with shape[15000,4094,64] and type float on /job:localhost/replica:0/task:0/device:CPU:0 by allocator cpu [Op:BiasAdd] name: \n\nCall arguments received by layer 'conv1d' (type Conv1D):\n  • inputs=tf.Tensor(shape=(15000, 4096, 1), dtype=float32)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 35\u001b[0m\n\u001b[0;32m     32\u001b[0m test_ims \u001b[38;5;241m=\u001b[39m test_image_vectors[np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mchoice(test_image_vectors\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m10\u001b[39m, replace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)]\n\u001b[0;32m     34\u001b[0m \u001b[38;5;66;03m# Compute SHAP values for the test dataset\u001b[39;00m\n\u001b[1;32m---> 35\u001b[0m shap_values \u001b[38;5;241m=\u001b[39m \u001b[43mexplainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshap_values\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_ims\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnsamples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m500\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     37\u001b[0m shap\u001b[38;5;241m.\u001b[39mforce_plot(explainer\u001b[38;5;241m.\u001b[39mexpected_value, shap_values, test_ims)\n\u001b[0;32m     39\u001b[0m \u001b[38;5;66;03m# # Summarize the SHAP values across all output classes\u001b[39;00m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;66;03m# shap_values_abs_mean = np.abs(shap_values.values).mean(0)\u001b[39;00m\n\u001b[0;32m     41\u001b[0m \n\u001b[0;32m     42\u001b[0m \u001b[38;5;66;03m# # Plot the SHAP values for the top features\u001b[39;00m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;66;03m# shap.summary_plot(shap_values_abs_mean, test_image_vectors, feature_names=[f'Feature {i}' for i in range(test_image_vectors.shape[1])])\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ray_rl\\lib\\site-packages\\shap\\explainers\\_kernel.py:242\u001b[0m, in \u001b[0;36mKernelExplainer.shap_values\u001b[1;34m(self, X, **kwargs)\u001b[0m\n\u001b[0;32m    240\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkeep_index:\n\u001b[0;32m    241\u001b[0m     data \u001b[38;5;241m=\u001b[39m convert_to_instance_with_index(data, column_name, index_value[i:i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m], index_name)\n\u001b[1;32m--> 242\u001b[0m explanations\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexplain(data, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs))\n\u001b[0;32m    243\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgc_collect\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m    244\u001b[0m     gc\u001b[38;5;241m.\u001b[39mcollect()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ray_rl\\lib\\site-packages\\shap\\explainers\\_kernel.py:436\u001b[0m, in \u001b[0;36mKernelExplainer.explain\u001b[1;34m(self, incoming_instance, **kwargs)\u001b[0m\n\u001b[0;32m    433\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkernelWeights[nfixed_samples:] \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m=\u001b[39m weight_left \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkernelWeights[nfixed_samples:]\u001b[38;5;241m.\u001b[39msum()\n\u001b[0;32m    435\u001b[0m \u001b[38;5;66;03m# execute the model on the synthetic samples we have created\u001b[39;00m\n\u001b[1;32m--> 436\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    438\u001b[0m \u001b[38;5;66;03m# solve then expand the feature importance (Shapley value) vector to contain the non-varying features\u001b[39;00m\n\u001b[0;32m    439\u001b[0m phi \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros((\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mgroups_size, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mD))\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ray_rl\\lib\\site-packages\\shap\\explainers\\_kernel.py:575\u001b[0m, in \u001b[0;36mKernelExplainer.run\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    573\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkeep_index_ordered:\n\u001b[0;32m    574\u001b[0m         data \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39msort_index()\n\u001b[1;32m--> 575\u001b[0m modelOut \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    576\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(modelOut, (pd\u001b[38;5;241m.\u001b[39mDataFrame, pd\u001b[38;5;241m.\u001b[39mSeries)):\n\u001b[0;32m    577\u001b[0m     modelOut \u001b[38;5;241m=\u001b[39m modelOut\u001b[38;5;241m.\u001b[39mvalues\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ray_rl\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ray_rl\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:6656\u001b[0m, in \u001b[0;36mraise_from_not_ok_status\u001b[1;34m(e, name)\u001b[0m\n\u001b[0;32m   6654\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mraise_from_not_ok_status\u001b[39m(e, name):\n\u001b[0;32m   6655\u001b[0m   e\u001b[38;5;241m.\u001b[39mmessage \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m name: \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(name \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m-> 6656\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_status_to_exception(e) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[1;31mResourceExhaustedError\u001b[0m: Exception encountered when calling layer 'conv1d' (type Conv1D).\n\n{{function_node __wrapped__BiasAdd_device_/job:localhost/replica:0/task:0/device:CPU:0}} OOM when allocating tensor with shape[15000,4094,64] and type float on /job:localhost/replica:0/task:0/device:CPU:0 by allocator cpu [Op:BiasAdd] name: \n\nCall arguments received by layer 'conv1d' (type Conv1D):\n  • inputs=tf.Tensor(shape=(15000, 4096, 1), dtype=float32)"
     ]
    }
   ],
   "source": [
    "# !pip install shap\n",
    "# !python -m pip install --upgrade pip\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Conv1D, MaxPooling1D, Flatten\n",
    "\n",
    "import shap\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Load the pre-trained model\n",
    "model = load_model('\"C:/Users/Owner/Downloads/gft_new_1dcnn_model.h5')\n",
    "\n",
    "# Load the test dataset\n",
    "# test_image_vectors, test_labels = load_images(os.path.join(dataset_path, 'Test'))\n",
    "# print(\"Test Dataset Loaded\")\n",
    "# Convert the labels to one-hot encoding\n",
    "# test_labels_one_hot = tf.keras.utils.to_categorical(test_labels, 2)\n",
    "def f(X):\n",
    "    return model.predict([X[i, :] for i in range(X.shape[0])]).flatten()\n",
    "\n",
    "test_image_vectors = np.load('C:/Users/Owner/Downloads/test_image_vectors.npy')\n",
    "# Define the background dataset for SHAP\n",
    "background_data = test_image_vectors[np.random.choice(test_image_vectors.shape[0], 30, replace=False)]\n",
    "\n",
    "# Initialize the SHAP explainer\n",
    "explainer = shap.KernelExplainer(model, background_data)\n",
    "\n",
    "test_ims = test_image_vectors[np.random.choice(test_image_vectors.shape[0], 10, replace=False)]\n",
    "\n",
    "# Compute SHAP values for the test dataset\n",
    "shap_values = explainer.shap_values(test_ims, nsamples=500)\n",
    "\n",
    "shap.force_plot(explainer.expected_value, shap_values, test_ims)\n",
    "\n",
    "# # Summarize the SHAP values across all output classes\n",
    "# shap_values_abs_mean = np.abs(shap_values.values).mean(0)\n",
    "\n",
    "# # Plot the SHAP values for the top features\n",
    "# shap.summary_plot(shap_values_abs_mean, test_image_vectors, feature_names=[f'Feature {i}' for i in range(test_image_vectors.shape[1])])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "814639c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1422, 4096)\n",
      "(380, 4096)\n"
     ]
    }
   ],
   "source": [
    "# Save the train_image_vectors array to a .npy file\n",
    "np.save('train_image_vectors.npy', train_image_vectors)\n",
    "\n",
    "# Save the test_image_vectors array to a .npy file\n",
    "np.save('test_image_vectors.npy', test_image_vectors)\n",
    "\n",
    "print(train_image_vectors.shape)\n",
    "print(test_image_vectors.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37c73207",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
